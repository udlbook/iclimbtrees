{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udlbook/iclimbtrees/blob/main/notebooks/DeepNN/DeepNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<svg width="764.45" height="63.759" version="1.1" viewBox="0 0 764.45 63.759" xmlns="http://www.w3.org/2000/svg">
 <g transform="matrix(.73548 0 0 .73548 0 3.388)" stroke-width="1.3597">
  <rect x="5e-7" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="10.330567" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="10.330567" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">I </tspan></text>
  <rect x="96" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="106.33057" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="106.33057" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">C </tspan></text>
  <rect x="180" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="190.33057" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="190.33057" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">L </tspan></text>
  <rect x="264" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="274.33057" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="274.33057" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">I </tspan></text>
  <rect x="348" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="358.33057" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="358.33057" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">M </tspan></text>
  <rect x="432" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="442.33057" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="442.33057" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">B </tspan></text>
  <g transform="translate(2 1.5376)">
   <rect x="624" y="-1.5376" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
   <text x="634.33057" y="64.224167" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="634.33057" y="64.224167" fill="#ffffff" font-family="Courier" stroke-width="1.3597">T </tspan></text>
   <rect x="708" y="-1.5376" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
   <text x="718.33057" y="64.224167" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="718.33057" y="64.224167" fill="#ffffff" font-family="Courier" stroke-width="1.3597">R </tspan></text>
   <rect x="792" y="-1.5376" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
   <text x="802.33057" y="64.224167" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="802.33057" y="64.224167" fill="#ffffff" font-family="Courier" stroke-width="1.3597">E</tspan></text>
   <rect x="876" y="-1.5376" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
   <text x="886.33057" y="64.224167" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="886.33057" y="64.224167" fill="#ffffff" font-family="Courier" stroke-width="1.3597">E </tspan></text>
   <rect x="960" y="-1.5376" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
   <text x="970.33057" y="64.224167" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="970.33057" y="64.224167" fill="#ffffff" font-family="Courier" stroke-width="1.3597">S </tspan></text>
  </g>
  <g transform="matrix(1.0499 0 0 1.0499 -28.092 -.27293)" fill="#4469d8" stroke="#fdffff">
   <rect x="528" y="-1.5376" width="77.387" height="77.387" ry="11.35" opacity=".98" stroke-width="5.18"/>
   <g transform="matrix(.74592 0 0 .74367 530.84 1.6744)" stroke-width="5.2162" featureKey="inlineSymbolFeature-0">
    <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
     <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
      <path d="m47.659 81.427c0.358-7.981 1.333-15.917 1.152-23.917-0.01-0.425-0.544-0.843-0.94-0.54-2.356 1.801-4.811 3.219-7.664 4.104-3.649 1.132-7.703-2.328-5.814-5.981 0.758-1.466 2.146-2.708 3.447-3.672 0.467-0.346 0.358-1.176-0.315-1.165-3.154 0.054-10.835 1.149-10.042-4.386 0.481-3.365 6.29-5.458 8.917-6.84 0.333-0.175 0.435-0.73 0.127-0.981-6.663-5.431-3.069-14.647 5.731-12.788 0.272 0.058 0.563-0.033 0.706-0.287 2.235-3.995 4.276-8.063 7.106-11.688-0.356-0.147-0.712-0.294-1.067-0.442 0.294 3.116 2.036 5.269 4.337 7.272 2.459 2.142 7.634 4.27 8.085 7.845 0.481 3.821-6.549 4.356-6.054 7.588 0.33 2.147 1.354 3.423 3.021 4.74 1.052 0.831 1.968 1.405 3.017 2.329 1.818 2.036 1.596 4.223-0.667 6.561-1.486 0.252-2.927 0.138-4.32-0.341-0.556-0.144-0.945 0.435-0.706 0.918 1.412 2.842 3.23 5.449 3.529 8.707 0.821 8.969-7.237 1.748-8.13 0.875-0.813-0.793-1.6-1.561-2.486-2.27-0.623-0.498-1.514 0.38-0.885 0.884 3.399 2.717 6.507 7.782 11.132 4.42 4.323-3.142-0.524-10.114-2.08-13.246-0.235 0.306-0.471 0.612-0.706 0.918 3.9 1.01 8.231 0.447 7.941-4.452-0.117-1.973-1.259-3.644-2.8-4.778-1.468-1.081-6.729-4.234-3.68-6.41 1.261-0.899 2.453-1.826 3.548-2.929 2.294-2.311 1.726-4.94-0.326-7.105-3.535-3.732-9.97-5.682-10.521-11.525-0.044-0.47-0.692-0.921-1.067-0.442-1.267 1.622-6.265 11.724-7.841 11.391-2.234-0.472-4.485 0.06-6.418 1.186-4.105 2.391-3.919 7.903-1.738 11.448 0.122 0.199 1.517 2.084 1.782 1.944-1.682 0.885-3.351 1.737-4.951 2.768-1.664 1.072-4.177 3.262-3.904 5.54 0.671 5.619 7.144 4.902 11.409 4.829-0.105-0.388-0.21-0.776-0.315-1.165-3.56 2.636-8.58 11.381-0.562 12.174 2.34 0.231 4.247-0.259 6.423-1.142 0.883-0.358 1.698-0.845 2.525-1.311 0.775-0.437 1.976-2.122 2.008-0.692 0.166 7.357-0.865 14.714-1.194 22.056-0.036 0.804 1.214 0.801 1.25-2e-3z" fill="#4469d8" stroke="#fdffff" stroke-linejoin="round" stroke-width="5.2162"/>
     </g>
     <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
      <path d="m22.301 83.156c-0.441-6.032-1.072-12.618 0.266-18.564 0.138-0.613-0.578-1.042-1.045-0.608-1.743 1.625-3.443 2.831-5.732 3.604-6.34-3.393-7.913-6.373-4.717-8.939 0.988-0.856 2.034-1.633 3.139-2.329 0.287-0.191 0.397-0.544 0.225-0.855-0.658-1.178-1.392-2.163-2.251-3.191-4.397-5.264-0.382-9.414 4.759-10.875 0.271-0.077 0.455-0.322 0.459-0.603 0.036-2.864 0.313-5.642 1.094-8.407 1.865-6.606 10.255-9.181 13.143-1.487 0.28 0.748 1.489 0.424 1.205-0.332-2.517-6.706-9.574-7.649-13.918-2.003-2.305 2.996-2.61 7.466-2.759 11.084-0.035 0.85-3.839 2.269-4.496 2.694-1.034 0.669-2.219 2.098-2.45 3.312-0.808 4.233 1.103 6.056 3.512 9.323 0.405 0.548-5.327 5.252-5.317 7.279 0.016 3.468 2.455 5.64 5.605 6.645 3.404 1.086 7.127-1.932 9.386-4.037-0.349-0.203-0.697-0.405-1.045-0.608-1.368 6.079-0.762 12.734-0.311 18.896 0.056 0.8 1.306 0.806 1.248 1e-3z" fill="#4469d8" stroke="#fdffff" stroke-linejoin="round" stroke-width="5.2162"/>
     </g>
     <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
      <path d="m21.424 64.741c1.983 2.707 4.981 4.199 8.349 3.637 3.594-0.6 5.191-4.13 5.291-7.411 0.024-0.807-1.226-0.804-1.25 0-0.202 6.67-7.523 8.313-11.31 3.143-0.472-0.643-1.557-0.02-1.08 0.631z" fill="#4469d8" stroke="#fdffff" stroke-width="5.2162"/>
     </g>
     <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
      <path d="m74.661 80.878c2.869-5.406 3.251-12.191 2.679-18.182-0.036-0.381-0.375-0.742-0.791-0.603-1.482 0.496-9.677 1.84-5.634-4.557 0.251-0.397-0.075-0.952-0.54-0.94-4.913 0.123-9.233-0.937-9.57-6.683-0.047-0.801-1.297-0.806-1.25 0 0.201 3.426 1.375 5.828 4.622 7.214 1.514 0.646 3.278 0.7 4.894 0.751-0.658-0.021-0.338 3.074-0.216 3.489 0.625 2.13 4.101 2.773 5.896 2.466 2.606-0.446 1.551 3.288 1.477 5.177-0.15 3.833-0.832 7.82-2.646 11.236-0.378 0.713 0.701 1.345 1.079 0.632z" fill="#4469d8" stroke="#fdffff" stroke-width="5.2162"/>
     </g>
     <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
      <path d="m76.881 63.299c3.341-0.618 7.425-1.372 7.423-5.67 0-1.473-0.141-3.462-1.403-4.486 0.524 0.425 2.703-1.287 3.381-1.885 5.097-4.499 1.607-12.585-4.301-13.85-0.222-0.047 2.216-4.5 2.515-5.157 0.832-1.834 0.614-3.634-8e-3 -5.472-1.133-3.347-6.327-9.06-10.153-9.283-1.411-0.082-2.449-0.077-3.515 0.881-1.212 1.09 0.842 3.98-1.963 2.484-4.82-2.573-5.125 2.25-7.856 4.852-0.584 0.557 0.301 1.439 0.885 0.884 1.199-1.143 0.961-0.736 1.574-2.026 2.202-4.641 4.768-2.589 7.178-1.388 0.334 0.167 0.839 0.047 0.918-0.374 0.208-1.098 0.205-1.025 0.186-2.169 2.787-1.84 5.084-1.596 6.891 0.731 0.745 0.715 1.449 1.469 2.113 2.261 4.874 5.507 2.097 8.833-0.535 13.968-0.228 0.445 0.06 0.897 0.54 0.94 8.368 0.749 8.684 11.983 0.698 13.757-0.432 0.096-0.64 0.75-0.276 1.044 4.99 4.046-0.386 7.969-4.622 8.753-0.794 0.147-0.458 1.351 0.33 1.205z" fill="#4469d8" stroke="#fdffff" stroke-linejoin="round" stroke-width="5.2162"/>
     </g>
    </g>
   </g>
  </g>
 </g>
</svg>
\">"
      ],
      "metadata": {
        "id": "A5pkEHVTiBv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep neural networks**\n",
        "\n",
        "This notebook investigates converting neural networks to matrix form.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and write code to complete the functions.\n",
        "\n",
        "You can save a local copy of this notebook in your Google account and work through it in Colab (recommended) or you can download the notebook and run it locally using Jupyter notebook or similar. If you are using CoLab, we recommend that turn off AI autocomplete (under cog icon in top-right corner), which will give you the answers and defeat the purpose of the exercise.\n",
        "\n",
        "A fully working version of this notebook with the complete answers can be found [here](https://colab.research.google.com/github/udlbook/iclimbtrees/blob/main/notebooks/DeepNN/DeepNetworks_Answers.ipynb).\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."
      ],
      "metadata": {
        "id": "MaKn8CFlzN8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ClURpZQzI6L"
      },
      "outputs": [],
      "source": [
        "# Imports math library\n",
        "import numpy as np\n",
        "# Imports plotting library\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ],
      "metadata": {
        "id": "YdmveeAUz4YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a shallow neural network with, one input, one output, and three hidden units\n",
        "def shallow_1_1_3(x, activation_fn, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
        "  # Initial lines\n",
        "  pre_1 = theta_10 + theta_11 * x\n",
        "  pre_2 = theta_20 + theta_21 * x\n",
        "  pre_3 = theta_30 + theta_31 * x\n",
        "  # Activation functions\n",
        "  act_1 = activation_fn(pre_1)\n",
        "  act_2 = activation_fn(pre_2)\n",
        "  act_3 = activation_fn(pre_3)\n",
        "  # Weight activations\n",
        "  w_act_1 = phi_1 * act_1\n",
        "  w_act_2 = phi_2 * act_2\n",
        "  w_act_3 = phi_3 * act_3\n",
        "  # Combine weighted activation and add y offset\n",
        "  y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
        "  # Return everything we have calculated\n",
        "  return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
      ],
      "metadata": {
        "id": "ximCLwIfz8kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot the shallow neural network.  We'll assume input in is range [-1,1] and output [-1,1]\n",
        "def plot_neural(x, y):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x.T,y.T)\n",
        "  ax.set_xlabel('Input'); ax.set_ylabel('Output')\n",
        "  ax.set_xlim([-1,1]);ax.set_ylim([-1,1])\n",
        "  ax.set_aspect(1.0)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "btrt7BX20gKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a network.  We'll just consider the inputs and outputs over the range [-1,1]."
      ],
      "metadata": {
        "id": "LxBJCObC-NTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets define some parameters and run the first neural network\n",
        "n1_theta_10 = 0.0   ; n1_theta_11 = -1.0\n",
        "n1_theta_20 = 0     ; n1_theta_21 = 1.0\n",
        "n1_theta_30 = -0.67 ; n1_theta_31 =  1.0\n",
        "n1_phi_0 = 1.0; n1_phi_1 = -2.0; n1_phi_2 = -3.0; n1_phi_3 = 9.3\n",
        "\n",
        "# Define a range of input values\n",
        "n1_in = np.arange(-1,1,0.01).reshape([1,-1])\n",
        "\n",
        "# We run the neural network for each of these input values\n",
        "n1_out, *_ = shallow_1_1_3(n1_in, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)\n",
        "# And then plot it\n",
        "plot_neural(n1_in, n1_out)"
      ],
      "metadata": {
        "id": "JRebvurv22pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll define the same neural network, but this time, we will  use matrix form as in equation 4.15.  When you get this right, it will draw the same plot as above."
      ],
      "metadata": {
        "id": "XCJqo_AjfAra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beta_0 = np.zeros((3,1))\n",
        "Omega_0 = np.zeros((3,1))\n",
        "beta_1 = np.zeros((1,1))\n",
        "Omega_1 = np.zeros((1,3))\n",
        "\n",
        "# TODO Fill in the values of the beta and Omega matrices with the n1_theta and n1_phi parameters that define the network above\n",
        "# !!! NOTE THAT MATRICES ARE CONVENTIONALLY INDEXED WITH a_11 IN THE TOP LEFT CORNER, BUT NDARRAYS START AT [0,0]\n",
        "# To get you started I've filled in a couple:\n",
        "beta_0[0,0] = n1_theta_10\n",
        "Omega_0[0,0] = n1_theta_11\n",
        "\n",
        "# Make sure that input data matrix has different inputs in its columns\n",
        "n_data = n1_in.size\n",
        "n_dim_in = 1\n",
        "n1_in_mat = np.reshape(n1_in,(n_dim_in,n_data))\n",
        "\n",
        "# This runs the network for ALL of the inputs, x at once so we can draw graph\n",
        "h1 = ReLU(beta_0 + np.matmul(Omega_0,n1_in_mat))\n",
        "n1_out = beta_1 + np.matmul(Omega_1,h1)\n",
        "\n",
        "# Draw the network and check that it looks the same as the non-matrix case\n",
        "plot_neural(n1_in, n1_out)"
      ],
      "metadata": {
        "id": "MR0AecZYfACR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll feed the output of the first network into the second one."
      ],
      "metadata": {
        "id": "qOcj2Rof-o20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets define some parameters and run the second neural network\n",
        "n2_theta_10 =  -0.6 ; n2_theta_11 = -1.0\n",
        "n2_theta_20 =  0.2  ; n2_theta_21 = 1.0\n",
        "n2_theta_30 =  -0.5  ; n2_theta_31 =  1.0\n",
        "n2_phi_0 = 0.5; n2_phi_1 = -1.0; n2_phi_2 = -1.5; n2_phi_3 = 2.0\n",
        "\n",
        "# Define a range of input values\n",
        "n2_in = np.arange(-1,1,0.01)\n",
        "\n",
        "# We run the second neural network on the output of the first network\n",
        "n2_out, *_ = \\\n",
        "    shallow_1_1_3(n1_out, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
        "# And then plot it\n",
        "plot_neural(n1_in, n2_out)"
      ],
      "metadata": {
        "id": "ZRjWu8i9239X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta_0 = np.zeros((3,1))\n",
        "Omega_0 = np.zeros((3,1))\n",
        "beta_1 = np.zeros((3,1))\n",
        "Omega_1 = np.zeros((3,3))\n",
        "beta_2 = np.zeros((1,1))\n",
        "Omega_2 = np.zeros((1,3))\n",
        "\n",
        "# TODO Fill in the values of the beta and Omega matrices for the n1_theta, n1_phi, n2_theta, and n2_phi parameters\n",
        "# that define the composition of the two networks above (see eqn 4.5 for Omega1 and beta1 albeit in different notation)\n",
        "# !!! NOTE THAT MATRICES ARE CONVENTIONALLY INDEXED WITH a_11 IN THE TOP LEFT CORNER, BUT NDARRAYS START AT [0,0] SO EVERYTHING IS OFFSET\n",
        "# To get you started I've filled in a few:\n",
        "beta_0[0,0] = n1_theta_10\n",
        "Omega_0[0,0] = n1_theta_11\n",
        "beta_1[0,0] = n2_theta_10 + n2_theta_11 * n1_phi_0\n",
        "Omega_1[0,0] = n2_theta_11 * n1_phi_1\n",
        "\n",
        "\n",
        "# Make sure that input data matrix has different inputs in its columns\n",
        "n_data = n1_in.size\n",
        "n_dim_in = 1\n",
        "n1_in_mat = np.reshape(n1_in,(n_dim_in,n_data))\n",
        "\n",
        "# This runs the network for ALL of the inputs, x at once so we can draw graph (hence extra np.ones term)\n",
        "h1 = ReLU(beta_0 + np.matmul(Omega_0,n1_in_mat))\n",
        "h2 = ReLU(beta_1 + np.matmul(Omega_1,h1))\n",
        "n1_out = beta_2 + np.matmul(Omega_2,h2)\n",
        "\n",
        "# Draw the network and check that it looks the same as the non-matrix version\n",
        "plot_neural(n1_in, n1_out)"
      ],
      "metadata": {
        "id": "ZB2HTalOE40X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's make a deep network with 3 hidden layers.  It will have $D_i=4$ inputs, $D_1=5$ neurons  in the first layer, $D_2=2$ neurons in the second layer and $D_3=4$ neurons in the third layer, and $D_o = 1$ output.  Consult figure 4.6 and equations 4.15 for guidance."
      ],
      "metadata": {
        "id": "0VANqxH2kyS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define sizes\n",
        "D_i=4; D_1=5; D_2=2; D_3=4; D_o=1\n",
        "# We'll choose the inputs and parameters of this network randomly using np.random.normal\n",
        "# For example, we'll set the input using\n",
        "n_data = 4;\n",
        "x = np.random.normal(size=(D_i, n_data))\n",
        "# TODO initialize the parameters randomly with the correct sizes\n",
        "# Replace the lines below\n",
        "beta_0 = np.random.normal(size=(1,1))\n",
        "Omega_0 = np.random.normal(size=(1,1))\n",
        "beta_1 = np.random.normal(size=(1,1))\n",
        "Omega_1 = np.random.normal(size=(1,1))\n",
        "beta_2 = np.random.normal(size=(1,1))\n",
        "Omega_2 = np.random.normal(size=(1,1))\n",
        "beta_3 = np.random.normal(size=(1,1))\n",
        "Omega_3 = np.random.normal(size=(1,1))\n",
        "\n",
        "\n",
        "# If you set the parameters to the correct sizes, the following code will run\n",
        "h1 = ReLU(beta_0 + np.matmul(Omega_0,x));\n",
        "h2 = ReLU(beta_1 + np.matmul(Omega_1,h1));\n",
        "h3 = ReLU(beta_2 + np.matmul(Omega_2,h2));\n",
        "y = beta_3 + np.matmul(Omega_3,h3)\n",
        "\n",
        "if h1.shape[0] is not D_1 or h1.shape[1] is not n_data:\n",
        "  print(\"h1 is wrong shape\")\n",
        "if h2.shape[0] is not D_2 or h1.shape[1] is not n_data:\n",
        "  print(\"h2 is wrong shape\")\n",
        "if h3.shape[0] is not D_3 or h1.shape[1] is not n_data:\n",
        "  print(\"h3 is wrong shape\")\n",
        "if y.shape[0] is not D_o or h1.shape[1] is not n_data:\n",
        "  print(\"Output is wrong shape\")\n",
        "\n",
        "# Print the inputs and outputs\n",
        "print(\"Input data points\")\n",
        "print(x)\n",
        "print (\"Output data points\")\n",
        "print(y)"
      ],
      "metadata": {
        "id": "RdBVAc_Rj22-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}